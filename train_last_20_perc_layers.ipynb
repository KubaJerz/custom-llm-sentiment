{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36e8274",
   "metadata": {},
   "source": [
    "# train_last_20_perc_layers \n",
    "\n",
    "So, in this notebook, we will freeze the first 80% of the layers (the embedding layer and the initial transformer blocks) since they likely capture basic features. Then, we will train the remaining 20% of the transformer layers along with the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, Gemma3Model,  TrainingArguments, Trainer\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c38933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we are using the pretrained model ( the model prior to SFT) since we have our own dataset\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "MODEL = \"google/gemma-3-4b-pt\"\n",
    "SEED = 69\n",
    "\n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get tha dataset\n",
    "# For us the dataset will be \n",
    "raw_dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "df_train = pd.DataFrame(raw_dataset['train'])\n",
    "df_test = pd.DataFrame(raw_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fe6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each segment of text \"tweet\" has a class 0 (negative), 1 (neutral), or 2 (positive)\n",
    "df_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this to format the input so model can understand\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of the tokenizer\n",
    "text = ['hello world', 'bobby like to eat pizza']\n",
    "vec = tokenizer(text, padding=True)\n",
    "print(\"encoding: \",vec)\n",
    "\n",
    "print(\"decoding: \",tokenizer.batch_decode(vec['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4808b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we jsut define this so be used with the 'dataset' map function so apply to the data\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data['text'], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokanizeion to the dataset\n",
    "dataset = raw_dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(len(dataset['train']) * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset and split into smaller part sow e can run on laptop\n",
    "train = dataset['train'].shuffle(SEED).select(range(int(len(dataset['train']) * 0.7)))\n",
    "dev = dataset['train'].shuffle(SEED).select(range(int(len(dataset['train']) * 0.7), len(dataset['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make data into a tensor\n",
    "X_train = torch.tensor(train['input_ids'])\n",
    "y_train = F.one_hot(torch.tensor(train['label']), num_classes=3).float()\n",
    "X_dev = torch.tensor(dev['input_ids'])\n",
    "y_dev = F.one_hot(torch.tensor(dev['label']), num_classes=3).float()\n",
    "\n",
    "X_train.shape, y_train.shape, X_dev.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "dev_dataset = TensorDataset(X_dev, y_dev)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"\\nGPU {i}:\")\n",
    "            print(f\"  Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Total: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are using gemma we need to add on to the base model a classification head\n",
    "# To do so we will import the base model then construct our model using output from the base model\n",
    "baseModel = Gemma3Model.from_pretrained(MODEL, device_map='auto', \n",
    "                                        output_hidden_states=True, \n",
    "                                        attn_implementation=\"eager\", \n",
    "                                        max_memory = {\n",
    "                                        0: \"20GiB\",        # GPU 0 - more memory training\n",
    "                                        1: \"8GiB\",        # GPU 1 - less of the model since it will have outpus and y \n",
    "                                        \"cpu\": \"80Gib\"\n",
    "                                        }\n",
    "                                        )\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for group in baseModel.parameters():\n",
    "    print(group.shape)\n",
    "    total += 1\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4fde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do this to see how many attention layer there are \n",
    "for group in baseModel.named_modules():\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see it has 33 attention layer so we will freeze the first 26\n",
    "\n",
    "#this wont effect that mem taken up on the GPU but lets freze the firs 80% of layers and leave the reast to train\n",
    "for param in baseModel.language_model.embed_tokens.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "max_layer_to_freeze = 26\n",
    "for i, layer in enumerate(baseModel.language_model.layers):\n",
    "    if i <= max_layer_to_freeze:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this so that we have more room on the gpus\n",
    "baseModel.vision_tower  = baseModel.vision_tower.to(\"cpu\")\n",
    "for param in baseModel.vision_tower.parameters():\n",
    "                param.requires_grad = False\n",
    "for param in baseModel.multi_modal_projector.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "    \n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel.config.output_hidden_states = True          \n",
    "baseModel.gradient_checkpointing_enable()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma3Classifier(nn.Module):\n",
    "    def __init__(self, bmodel, hiddensize, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bmodel = bmodel\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.head = nn.Linear(hiddensize, 3).to('cuda:1')\n",
    "        self.device_placement = True\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        out = self.bmodel(input_ids)\n",
    "        hidden_state = out.hidden_states[-1]\n",
    "        embeddings = hidden_state[:, -1, :]  \n",
    "\n",
    "        embeddings = embeddings.to('cuda:1')\n",
    "\n",
    "        logits = self.head(self.dropout(embeddings))\n",
    "\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5078d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gemma3Classifier(bmodel=baseModel, dropout=0.1, hiddensize=baseModel.config.text_config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters() ,lr=0.0003)\n",
    "lossi = []\n",
    "devlossi = []\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accumulation_steps = 8 # 8 * 4(our small batch size due to mem constraints) = 32 new updates after\n",
    "for epoch in tqdm(range(10)):\n",
    "    model.train()\n",
    "\n",
    "    loss_total = 0\n",
    "    for i,  (X_train, y_train) in enumerate(train_loader):\n",
    "        out = model(input_ids=X_train)\n",
    "        y_train = y_train.to('cuda:1')\n",
    "        loss = criterion(out, y_train)\n",
    "        loss_total += loss.item() \n",
    "\n",
    "        loss = loss / accumulation_steps #since batch size is jsut 4\n",
    "        loss.backward()\n",
    "\n",
    "        # now we upadte every {accumulation_steps} \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # if not perfectl;y diviable the we have left over gradient we need to use\n",
    "    if (i + 1) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()        \n",
    "\n",
    "    lossi.append(loss_total / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    dev_loss_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_dev, y_dev in dev_loader:\n",
    "            out = model(input_ids=X_dev)\n",
    "            y_dev = y_dev.to('cuda:1')\n",
    "            loss = criterion(out, y_dev)\n",
    "            dev_loss_total += loss.item()\n",
    "\n",
    "    devlossi.append(dev_loss_total / len(dev_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi, label=\"lossi\")\n",
    "plt.plot(devlossi, label=\"devlossi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d518970",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = dataset['train'][4]\n",
    "ex_text = ex['text']\n",
    "ex_input = torch.tensor(ex['input_ids']).unsqueeze(dim=0)\n",
    "ex_label = ex['label']\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(ex_input)\n",
    "\n",
    "print(f'The test is: {ex_text}')\n",
    "if ex_label == 0:\n",
    "    print(f'The label is: [1, 0, 0]')\n",
    "elif ex_label == 1:\n",
    "    print(f'The label is: [0, 1, 0]')\n",
    "else:\n",
    "    print(f'The label is: [0, 0, 1]')\n",
    "\n",
    "print(f'The pred is: {torch.softmax(pred, dim=1)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
