{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36e8274",
   "metadata": {},
   "source": [
    "# train_last_20_perc_layers \n",
    "\n",
    "So, in this notebook, we will freeze the first 80% of the layers (the embedding layer and the initial transformer blocks) since they likely capture basic features. Then, we will train the remaining 20% of the transformer layers along with the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1324b8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.virenv/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, Gemma3Model,  TrainingArguments, Trainer\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c38933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we are using the pretrained model ( the model prior to SFT) since we have our own dataset\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "MODEL = \"google/gemma-3-4b-pt\"\n",
    "SEED = 69\n",
    "\n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242f85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get tha dataset\n",
    "# For us the dataset will be \n",
    "raw_dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "df_train = pd.DataFrame(raw_dataset['train'])\n",
    "df_test = pd.DataFrame(raw_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158fe6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each segment of text \"tweet\" has a class 0 (negative), 1 (neutral), or 2 (positive)\n",
    "df_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edd47f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26727</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26728</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26729</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26730</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26731</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26732 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  label  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going      1   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n",
       "2      088c60f138                          my boss is bullying me...      0   \n",
       "3      9642c003ef                     what interview! leave me alone      0   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n",
       "...           ...                                                ...    ...   \n",
       "26727  4eac33d1c0   wish we could come see u on Denver  husband l...      0   \n",
       "26728  4f4c4fc327   I`ve wondered about rake to.  The client has ...      0   \n",
       "26729  f67aae2310   Yay good for both of you. Enjoy the break - y...      2   \n",
       "26730  ed167662a5                         But it was worth it  ****.      2   \n",
       "26731  6f7127d9d7     All this flirting going on - The ATG smiles...      1   \n",
       "\n",
       "      label_text  \n",
       "0        neutral  \n",
       "1       negative  \n",
       "2       negative  \n",
       "3       negative  \n",
       "4       negative  \n",
       "...          ...  \n",
       "26727   negative  \n",
       "26728   negative  \n",
       "26729   positive  \n",
       "26730   positive  \n",
       "26731    neutral  \n",
       "\n",
       "[26732 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feed4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this to format the input so model can understand\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd20ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding:  {'input_ids': [[0, 0, 0, 0, 2, 23391, 1902], [2, 236763, 13990, 1133, 531, 9039, 19406]], 'attention_mask': [[0, 0, 0, 0, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n",
      "decoding:  ['<pad><pad><pad><pad><bos>hello world', '<bos>bobby like to eat pizza']\n"
     ]
    }
   ],
   "source": [
    "# test of the tokenizer\n",
    "text = ['hello world', 'bobby like to eat pizza']\n",
    "vec = tokenizer(text, padding=True)\n",
    "print(\"encoding: \",vec)\n",
    "\n",
    "print(\"decoding: \",tokenizer.batch_decode(vec['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed4808b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we jsut define this so be used with the 'dataset' map function so apply to the data\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data['text'], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94fe379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokanizeion to the dataset\n",
    "dataset = raw_dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0052a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset and split into smaller part sow e can run on laptop\n",
    "train = dataset['train'].shuffle(SEED).select(range(100))\n",
    "test = dataset['test'].shuffle(SEED).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6cd2b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 128]),\n",
       " torch.Size([100, 3]),\n",
       " torch.Size([100, 128]),\n",
       " torch.Size([100, 3]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make data into a tensor\n",
    "X_train = torch.tensor(train['input_ids'])\n",
    "y_train = F.one_hot(torch.tensor(train['label']), num_classes=3).float()\n",
    "X_test = torch.tensor(test['input_ids'])\n",
    "y_test = F.one_hot(torch.tensor(test['label']), num_classes=3).float()\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63de95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1ae67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"\\nGPU {i}:\")\n",
    "            print(f\"  Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Total: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f31a9ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.28s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU 0:\n",
      "  Allocated: 7.93 GB\n",
      "  Cached: 7.94 GB\n",
      "  Total: 23.67 GB\n",
      "\n",
      "GPU 1:\n",
      "  Allocated: 7.74 GB\n",
      "  Cached: 7.74 GB\n",
      "  Total: 23.67 GB\n"
     ]
    }
   ],
   "source": [
    "# Since we are using gemma we need to add on to the base model a classification head\n",
    "# To do so we will import the base model then construct our model using output from the base model\n",
    "baseModel = Gemma3Model.from_pretrained(MODEL, device_map='auto', \n",
    "                                        output_hidden_states=True, \n",
    "                                        attn_implementation=\"eager\", \n",
    "                                        max_memory = {\n",
    "                                        0: \"20GiB\",        # GPU 0 - more memory training\n",
    "                                        1: \"8GiB\",        # GPU 1 - less of the model since it will have outpus and y \n",
    "                                        \"cpu\": \"80Gib\"\n",
    "                                        }\n",
    "                                        )\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bf7807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1152, 3, 14, 14])\n",
      "torch.Size([1152])\n",
      "torch.Size([4096, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 2560])\n",
      "torch.Size([1152])\n",
      "torch.Size([262208, 2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "883\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for group in baseModel.parameters():\n",
    "    print(group.shape)\n",
    "    total += 1\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d4fde94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', Gemma3Model(\n",
      "  (vision_tower): SiglipVisionModel(\n",
      "    (vision_model): SiglipVisionTransformer(\n",
      "      (embeddings): SiglipVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "        (position_embedding): Embedding(4096, 1152)\n",
      "      )\n",
      "      (encoder): SiglipEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-26): 27 x SiglipEncoderLayer(\n",
      "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (self_attn): SiglipAttention(\n",
      "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): SiglipMLP(\n",
      "              (activation_fn): PytorchGELUTanh()\n",
      "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "  )\n",
      "  (language_model): Gemma3TextModel(\n",
      "    (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-33): 34 x Gemma3DecoderLayer(\n",
      "        (self_attn): Gemma3Attention(\n",
      "          (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Gemma3MLP(\n",
      "          (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "          (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "          (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    (rotary_emb): Gemma3RotaryEmbedding()\n",
      "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "  )\n",
      "))\n",
      "('vision_tower', SiglipVisionModel(\n",
      "  (vision_model): SiglipVisionTransformer(\n",
      "    (embeddings): SiglipVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "      (position_embedding): Embedding(4096, 1152)\n",
      "    )\n",
      "    (encoder): SiglipEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-26): 27 x SiglipEncoderLayer(\n",
      "          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          (self_attn): SiglipAttention(\n",
      "            (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): SiglipMLP(\n",
      "            (activation_fn): PytorchGELUTanh()\n",
      "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model', SiglipVisionTransformer(\n",
      "  (embeddings): SiglipVisionEmbeddings(\n",
      "    (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "    (position_embedding): Embedding(4096, 1152)\n",
      "  )\n",
      "  (encoder): SiglipEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-26): 27 x SiglipEncoderLayer(\n",
      "        (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attn): SiglipAttention(\n",
      "          (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): SiglipMLP(\n",
      "          (activation_fn): PytorchGELUTanh()\n",
      "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "))\n",
      "('vision_tower.vision_model.embeddings', SiglipVisionEmbeddings(\n",
      "  (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "  (position_embedding): Embedding(4096, 1152)\n",
      "))\n",
      "('vision_tower.vision_model.embeddings.patch_embedding', Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid))\n",
      "('vision_tower.vision_model.embeddings.position_embedding', Embedding(4096, 1152))\n",
      "('vision_tower.vision_model.encoder', SiglipEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0-26): 27 x SiglipEncoderLayer(\n",
      "      (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attn): SiglipAttention(\n",
      "        (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): SiglipMLP(\n",
      "        (activation_fn): PytorchGELUTanh()\n",
      "        (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "        (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers', ModuleList(\n",
      "  (0-26): 27 x SiglipEncoderLayer(\n",
      "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attn): SiglipAttention(\n",
      "      (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "      (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "      (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "      (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    )\n",
      "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): SiglipMLP(\n",
      "      (activation_fn): PytorchGELUTanh()\n",
      "      (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "      (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "    )\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.0', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.0.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.0.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.0.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.0.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.1.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.1.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.1.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.1.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.2.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.2.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.2.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.2.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.3.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.3.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.3.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.3.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.4.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.4.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.4.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.4.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.5.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.5.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.5.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.5.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.6.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.6.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.6.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.6.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.7.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.7.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.7.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.7.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.8.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.8.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.8.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.8.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.9.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.9.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.9.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.9.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.10.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.10.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.10.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.10.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.11.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.11.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.11.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.11.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.12.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.12.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.12.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.12.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.13.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.13.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.13.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.13.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.14.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.14.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.14.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.14.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.15.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.15.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.15.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.15.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.16.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.16.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.16.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.16.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.17.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.17.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.17.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.17.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.18.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.18.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.18.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.18.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.19.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.19.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.19.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.19.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.20.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.20.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.20.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.20.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.21.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.21.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.21.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.21.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.22.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.22.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.22.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.22.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.23.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.23.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.23.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.23.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.24.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.24.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.24.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.24.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.25.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.25.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.25.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.25.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.26.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.26.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.26.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.26.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.post_layernorm', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('multi_modal_projector', Gemma3MultiModalProjector(\n",
      "  (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "))\n",
      "('multi_modal_projector.mm_soft_emb_norm', Gemma3RMSNorm((1152,), eps=1e-06))\n",
      "('multi_modal_projector.avg_pool', AvgPool2d(kernel_size=4, stride=4, padding=0))\n",
      "('language_model', Gemma3TextModel(\n",
      "  (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-33): 34 x Gemma3DecoderLayer(\n",
      "      (self_attn): Gemma3Attention(\n",
      "        (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "        (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Gemma3MLP(\n",
      "        (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "        (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "        (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "        (act_fn): PytorchGELUTanh()\n",
      "      )\n",
      "      (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (rotary_emb): Gemma3RotaryEmbedding()\n",
      "  (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "))\n",
      "('language_model.embed_tokens', Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0))\n",
      "('language_model.layers', ModuleList(\n",
      "  (0-33): 34 x Gemma3DecoderLayer(\n",
      "    (self_attn): Gemma3Attention(\n",
      "      (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "      (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    )\n",
      "    (mlp): Gemma3MLP(\n",
      "      (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "      (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "      (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "      (act_fn): PytorchGELUTanh()\n",
      "    )\n",
      "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  )\n",
      "))\n",
      "('language_model.layers.0', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.0.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.0.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.0.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.0.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.0.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.0.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.0.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.0.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.0.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.0.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.0.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.0.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.0.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.0.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.0.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.0.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.1', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.1.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.1.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.1.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.1.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.1.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.1.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.1.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.1.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.1.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.1.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.1.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.1.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.1.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.1.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.1.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.1.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.2', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.2.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.2.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.2.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.2.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.2.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.2.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.2.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.2.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.2.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.2.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.2.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.2.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.2.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.2.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.2.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.2.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.3', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.3.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.3.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.3.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.3.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.3.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.3.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.3.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.3.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.3.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.3.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.3.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.3.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.3.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.3.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.3.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.3.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.4', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.4.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.4.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.4.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.4.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.4.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.4.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.4.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.4.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.4.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.4.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.4.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.4.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.4.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.4.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.4.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.4.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.5', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.5.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.5.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.5.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.5.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.5.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.5.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.5.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.5.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.5.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.5.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.5.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.5.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.5.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.5.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.5.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.5.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.6', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.6.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.6.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.6.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.6.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.6.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.6.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.6.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.6.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.6.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.6.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.6.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.6.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.6.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.6.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.6.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.6.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.7', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.7.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.7.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.7.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.7.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.7.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.7.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.7.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.7.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.7.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.7.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.7.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.7.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.7.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.7.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.7.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.7.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.8', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.8.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.8.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.8.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.8.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.8.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.8.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.8.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.8.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.8.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.8.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.8.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.8.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.8.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.8.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.8.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.8.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.9', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.9.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.9.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.9.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.9.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.9.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.9.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.9.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.9.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.9.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.9.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.9.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.9.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.9.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.9.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.9.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.9.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.10', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.10.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.10.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.10.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.10.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.10.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.10.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.10.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.10.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.10.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.10.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.10.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.10.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.10.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.10.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.10.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.10.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.11', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.11.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.11.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.11.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.11.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.11.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.11.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.11.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.11.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.11.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.11.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.11.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.11.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.11.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.11.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.11.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.11.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.12', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.12.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.12.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.12.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.12.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.12.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.12.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.12.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.12.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.12.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.12.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.12.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.12.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.12.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.12.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.12.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.12.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.13', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.13.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.13.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.13.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.13.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.13.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.13.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.13.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.13.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.13.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.13.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.13.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.13.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.13.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.13.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.13.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.13.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.14', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.14.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.14.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.14.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.14.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.14.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.14.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.14.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.14.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.14.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.14.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.14.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.14.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.14.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.14.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.14.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.14.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.15', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.15.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.15.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.15.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.15.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.15.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.15.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.15.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.15.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.15.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.15.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.15.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.15.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.15.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.15.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.15.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.15.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.16', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.16.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.16.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.16.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.16.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.16.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.16.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.16.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.16.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.16.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.16.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.16.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.16.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.16.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.16.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.16.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.16.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.17', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.17.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.17.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.17.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.17.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.17.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.17.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.17.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.17.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.17.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.17.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.17.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.17.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.17.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.17.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.17.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.17.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.18', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.18.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.18.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.18.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.18.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.18.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.18.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.18.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.18.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.18.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.18.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.18.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.18.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.18.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.18.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.18.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.18.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.19', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.19.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.19.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.19.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.19.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.19.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.19.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.19.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.19.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.19.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.19.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.19.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.19.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.19.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.19.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.19.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.19.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.20', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.20.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.20.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.20.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.20.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.20.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.20.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.20.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.20.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.20.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.20.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.20.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.20.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.20.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.20.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.20.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.20.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.21', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.21.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.21.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.21.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.21.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.21.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.21.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.21.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.21.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.21.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.21.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.21.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.21.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.21.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.21.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.21.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.21.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.22', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.22.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.22.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.22.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.22.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.22.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.22.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.22.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.22.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.22.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.22.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.22.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.22.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.22.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.22.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.22.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.22.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.23', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.23.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.23.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.23.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.23.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.23.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.23.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.23.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.23.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.23.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.23.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.23.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.23.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.23.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.23.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.23.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.23.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.24', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.24.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.24.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.24.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.24.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.24.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.24.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.24.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.24.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.24.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.24.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.24.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.24.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.24.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.24.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.24.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.24.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.25', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.25.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.25.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.25.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.25.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.25.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.25.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.25.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.25.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.25.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.25.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.25.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.25.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.25.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.25.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.25.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.25.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.26', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.26.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.26.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.26.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.26.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.26.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.26.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.26.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.26.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.26.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.26.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.26.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.26.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.26.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.26.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.26.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.26.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.27', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.27.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.27.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.27.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.27.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.27.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.27.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.27.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.27.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.27.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.27.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.27.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.27.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.27.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.27.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.27.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.27.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.28', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.28.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.28.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.28.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.28.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.28.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.28.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.28.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.28.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.28.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.28.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.28.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.28.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.28.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.28.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.28.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.28.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.29', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.29.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.29.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.29.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.29.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.29.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.29.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.29.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.29.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.29.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.29.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.29.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.29.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.29.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.29.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.29.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.29.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.30', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.30.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.30.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.30.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.30.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.30.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.30.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.30.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.30.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.30.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.30.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.30.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.30.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.30.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.30.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.30.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.30.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.31', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.31.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.31.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.31.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.31.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.31.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.31.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.31.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.31.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.31.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.31.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.31.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.31.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.31.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.31.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.31.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.31.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.32', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.32.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.32.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.32.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.32.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.32.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.32.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.32.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.32.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.32.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.32.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.32.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.32.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.32.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.32.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.32.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.32.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.33', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.33.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.33.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.33.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.33.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.33.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.33.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.33.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.33.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.33.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.33.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.33.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.33.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.33.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.33.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.33.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.33.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.norm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.rotary_emb', Gemma3RotaryEmbedding())\n",
      "('language_model.rotary_emb_local', Gemma3RotaryEmbedding())\n"
     ]
    }
   ],
   "source": [
    "# we do this to see how many attention layer there are \n",
    "for group in baseModel.named_modules():\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73fc710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see it has 33 attention layer so we will freeze the first 26\n",
    "\n",
    "#this wont effect that mem taken up on the GPU but lets freze the firs 80% of layers and leave the reast to train\n",
    "for param in baseModel.language_model.embed_tokens.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "max_layer_to_freeze = 26\n",
    "for i, layer in enumerate(baseModel.language_model.layers):\n",
    "    if i <= max_layer_to_freeze:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1c1e3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU 0:\n",
      "  Allocated: 6.38 GB\n",
      "  Cached: 7.94 GB\n",
      "  Total: 23.67 GB\n",
      "\n",
      "GPU 1:\n",
      "  Allocated: 7.74 GB\n",
      "  Cached: 7.74 GB\n",
      "  Total: 23.67 GB\n"
     ]
    }
   ],
   "source": [
    "# We do this so that we have more room on the gpus\n",
    "baseModel.vision_tower  = baseModel.vision_tower.to(\"cpu\")\n",
    "for param in baseModel.vision_tower.parameters():\n",
    "                param.requires_grad = False\n",
    "for param in baseModel.multi_modal_projector.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "    \n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b8b7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel.config.output_hidden_states = True          \n",
    "baseModel.gradient_checkpointing_enable()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2e4264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma3Classifier(nn.Module):\n",
    "    def __init__(self, bmodel, hiddensize, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bmodel = bmodel\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.head = nn.Linear(hiddensize, 3).to('cuda:1')\n",
    "        self.device_placement = True\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        out = self.bmodel(input_ids)\n",
    "        hidden_state = out.hidden_states[-1]\n",
    "        embeddings = hidden_state[:, -1, :]  \n",
    "\n",
    "        embeddings = embeddings.to('cuda:1')\n",
    "\n",
    "        logits = self.head(self.dropout(embeddings))\n",
    "\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5078d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gemma3Classifier(bmodel=baseModel, dropout=0.1, hiddensize=baseModel.config.text_config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d9c1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters() ,lr=0.0003)\n",
    "lossi = []\n",
    "devlossi = []\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf5e96f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:07<00:00, 18.79s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(10)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss_total = 0\n",
    "    for X_train, y_train in train_loader:\n",
    "        out = model(input_ids=X_train)\n",
    "        y_train = y_train.to('cuda:1')\n",
    "        loss = criterion(out, y_train)\n",
    "        loss.backward()\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    optimizer.step()\n",
    "    lossi.append(loss_total / len(train_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_loss_total = 0\n",
    "        for X_test, y_test in test_loader:\n",
    "            out = model(input_ids=X_test)\n",
    "            y_test = y_test.to('cuda:1')\n",
    "            loss = criterion(out, y_test)\n",
    "            dev_loss_total += loss.item()\n",
    "\n",
    "        devlossi.append(dev_loss_total / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfdb6824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x789d2b7405c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi, label=\"lossi\")\n",
    "plt.plot(devlossi, label=\"devlossi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d518970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test is: :visiting my friendster and facebook\n",
      "The label is: [0, 1, 0]\n",
      "The pred is: tensor([[0.1678, 0.8187, 0.0134]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "ex = dataset['train'][44]\n",
    "ex_text = ex['text']\n",
    "ex_input = torch.tensor(ex['input_ids']).unsqueeze(dim=0)\n",
    "ex_label = ex['label']\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(ex_input)\n",
    "\n",
    "print(f'The test is: {ex_text}')\n",
    "if ex_label == 0:\n",
    "    print(f'The label is: [1, 0, 0]')\n",
    "elif ex_label == 1:\n",
    "    print(f'The label is: [0, 1, 0]')\n",
    "else:\n",
    "    print(f'The label is: [0, 0, 1]')\n",
    "\n",
    "print(f'The pred is: {torch.softmax(pred, dim=1)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
