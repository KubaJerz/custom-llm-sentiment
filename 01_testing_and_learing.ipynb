{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36e8274",
   "metadata": {},
   "source": [
    "# train_last_20_perc_layers \n",
    "\n",
    "So, in this notebook, we will freeze the first 80% of the layers (the embedding layer and the initial transformer blocks) since they likely capture basic features. Then, we will train the remaining 20% of the transformer layers along with the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324b8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.virenv/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, Gemma3Model,  TrainingArguments, Trainer\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c38933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we are using the pretrained model ( the model prior to SFT) since we have our own dataset\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "MODEL = \"google/gemma-3-4b-pt\"\n",
    "SEED = 69\n",
    "\n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get tha dataset\n",
    "# For us the dataset will be \n",
    "raw_dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "df_train = pd.DataFrame(raw_dataset['train'])\n",
    "df_test = pd.DataFrame(raw_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fe6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each segment of text \"tweet\" has a class 0 (negative), 1 (neutral), or 2 (positive)\n",
    "df_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd47f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26727</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26728</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26729</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26730</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26731</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26732 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  label  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going      1   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n",
       "2      088c60f138                          my boss is bullying me...      0   \n",
       "3      9642c003ef                     what interview! leave me alone      0   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n",
       "...           ...                                                ...    ...   \n",
       "26727  4eac33d1c0   wish we could come see u on Denver  husband l...      0   \n",
       "26728  4f4c4fc327   I`ve wondered about rake to.  The client has ...      0   \n",
       "26729  f67aae2310   Yay good for both of you. Enjoy the break - y...      2   \n",
       "26730  ed167662a5                         But it was worth it  ****.      2   \n",
       "26731  6f7127d9d7     All this flirting going on - The ATG smiles...      1   \n",
       "\n",
       "      label_text  \n",
       "0        neutral  \n",
       "1       negative  \n",
       "2       negative  \n",
       "3       negative  \n",
       "4       negative  \n",
       "...          ...  \n",
       "26727   negative  \n",
       "26728   negative  \n",
       "26729   positive  \n",
       "26730   positive  \n",
       "26731    neutral  \n",
       "\n",
       "[26732 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this to format the input so model can understand\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding:  {'input_ids': [[0, 0, 0, 0, 2, 23391, 1902], [2, 236763, 13990, 1133, 531, 9039, 19406]], 'attention_mask': [[0, 0, 0, 0, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n",
      "decoding:  ['<pad><pad><pad><pad><bos>hello world', '<bos>bobby like to eat pizza']\n"
     ]
    }
   ],
   "source": [
    "# test of the tokenizer\n",
    "text = ['hello world', 'bobby like to eat pizza']\n",
    "vec = tokenizer(text, padding=True)\n",
    "print(\"encoding: \",vec)\n",
    "\n",
    "print(\"decoding: \",tokenizer.batch_decode(vec['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4808b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we jsut define this so be used with the 'dataset' map function so apply to the data\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data['text'], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokanizeion to the dataset\n",
    "dataset = raw_dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870eba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18712"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(dataset['train']) * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset and split into smaller part sow e can run on laptop\n",
    "train = dataset['train'].shuffle(SEED).select(range(int(len(dataset['train']) * 0.7)))\n",
    "dev = dataset['train'].shuffle(SEED).select(range(int(len(dataset['train']) * 0.7), len(dataset['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd2b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([18712, 128]),\n",
       " torch.Size([18712, 3]),\n",
       " torch.Size([8020, 128]),\n",
       " torch.Size([8020, 3]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make data into a tensor\n",
    "X_train = torch.tensor(train['input_ids'])\n",
    "y_train = F.one_hot(torch.tensor(train['label']), num_classes=3).float()\n",
    "X_dev = torch.tensor(dev['input_ids'])\n",
    "y_dev = F.one_hot(torch.tensor(dev['label']), num_classes=3).float()\n",
    "\n",
    "X_train.shape, y_train.shape, X_dev.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "dev_dataset = TensorDataset(X_dev, y_dev)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"\\nGPU {i}:\")\n",
    "            print(f\"  Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Total: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a9ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU 0:\n",
      "  Allocated: 7.93 GB\n",
      "  Cached: 7.94 GB\n",
      "  Total: 23.67 GB\n",
      "\n",
      "GPU 1:\n",
      "  Allocated: 7.74 GB\n",
      "  Cached: 7.74 GB\n",
      "  Total: 23.67 GB\n"
     ]
    }
   ],
   "source": [
    "# Since we are using gemma we need to add on to the base model a classification head\n",
    "# To do so we will import the base model then construct our model using output from the base model\n",
    "baseModel = Gemma3Model.from_pretrained(MODEL, device_map='auto', \n",
    "                                        output_hidden_states=True, \n",
    "                                        attn_implementation=\"eager\", \n",
    "                                        max_memory = {\n",
    "                                        0: \"20GiB\",        # GPU 0 - more memory training\n",
    "                                        1: \"8GiB\",        # GPU 1 - less of the model since it will have outpus and y \n",
    "                                        \"cpu\": \"80Gib\"\n",
    "                                        }\n",
    "                                        )\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1152, 3, 14, 14])\n",
      "torch.Size([1152])\n",
      "torch.Size([4096, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([4304, 1152])\n",
      "torch.Size([4304])\n",
      "torch.Size([1152, 4304])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 2560])\n",
      "torch.Size([1152])\n",
      "torch.Size([262208, 2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2048, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([1024, 2560])\n",
      "torch.Size([2560, 2048])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([10240, 2560])\n",
      "torch.Size([2560, 10240])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n",
      "883\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for group in baseModel.parameters():\n",
    "    print(group.shape)\n",
    "    total += 1\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4fde94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', Gemma3Model(\n",
      "  (vision_tower): SiglipVisionModel(\n",
      "    (vision_model): SiglipVisionTransformer(\n",
      "      (embeddings): SiglipVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "        (position_embedding): Embedding(4096, 1152)\n",
      "      )\n",
      "      (encoder): SiglipEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-26): 27 x SiglipEncoderLayer(\n",
      "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (self_attn): SiglipAttention(\n",
      "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): SiglipMLP(\n",
      "              (activation_fn): PytorchGELUTanh()\n",
      "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "  )\n",
      "  (language_model): Gemma3TextModel(\n",
      "    (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-33): 34 x Gemma3DecoderLayer(\n",
      "        (self_attn): Gemma3Attention(\n",
      "          (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Gemma3MLP(\n",
      "          (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "          (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "          (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    (rotary_emb): Gemma3RotaryEmbedding()\n",
      "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "  )\n",
      "))\n",
      "('vision_tower', SiglipVisionModel(\n",
      "  (vision_model): SiglipVisionTransformer(\n",
      "    (embeddings): SiglipVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "      (position_embedding): Embedding(4096, 1152)\n",
      "    )\n",
      "    (encoder): SiglipEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-26): 27 x SiglipEncoderLayer(\n",
      "          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          (self_attn): SiglipAttention(\n",
      "            (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): SiglipMLP(\n",
      "            (activation_fn): PytorchGELUTanh()\n",
      "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model', SiglipVisionTransformer(\n",
      "  (embeddings): SiglipVisionEmbeddings(\n",
      "    (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "    (position_embedding): Embedding(4096, 1152)\n",
      "  )\n",
      "  (encoder): SiglipEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-26): 27 x SiglipEncoderLayer(\n",
      "        (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attn): SiglipAttention(\n",
      "          (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): SiglipMLP(\n",
      "          (activation_fn): PytorchGELUTanh()\n",
      "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "))\n",
      "('vision_tower.vision_model.embeddings', SiglipVisionEmbeddings(\n",
      "  (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "  (position_embedding): Embedding(4096, 1152)\n",
      "))\n",
      "('vision_tower.vision_model.embeddings.patch_embedding', Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid))\n",
      "('vision_tower.vision_model.embeddings.position_embedding', Embedding(4096, 1152))\n",
      "('vision_tower.vision_model.encoder', SiglipEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0-26): 27 x SiglipEncoderLayer(\n",
      "      (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attn): SiglipAttention(\n",
      "        (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): SiglipMLP(\n",
      "        (activation_fn): PytorchGELUTanh()\n",
      "        (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "        (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers', ModuleList(\n",
      "  (0-26): 27 x SiglipEncoderLayer(\n",
      "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attn): SiglipAttention(\n",
      "      (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "      (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "      (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "      (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    )\n",
      "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): SiglipMLP(\n",
      "      (activation_fn): PytorchGELUTanh()\n",
      "      (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "      (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "    )\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.0', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.0.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.0.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.0.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.0.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.0.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.1.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.1.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.1.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.1.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.1.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.2.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.2.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.2.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.2.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.2.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.3.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.3.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.3.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.3.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.3.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.4.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.4.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.4.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.4.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.4.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.5.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.5.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.5.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.5.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.5.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.6.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.6.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.6.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.6.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.6.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.7.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.7.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.7.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.7.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.7.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.8.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.8.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.8.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.8.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.8.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.9.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.9.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.9.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.9.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.9.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.10.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.10.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.10.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.10.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.10.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.11.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.11.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.11.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.11.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.11.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.12.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.12.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.12.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.12.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.12.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.13.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.13.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.13.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.13.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.13.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.14.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.14.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.14.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.14.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.14.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.15.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.15.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.15.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.15.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.15.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.16.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.16.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.16.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.16.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.16.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.17.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.17.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.17.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.17.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.17.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.18.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.18.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.18.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.18.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.18.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.19.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.19.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.19.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.19.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.19.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.20.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.20.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.20.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.20.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.20.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.21.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.21.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.21.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.21.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.21.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.22.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.22.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.22.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.22.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.22.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.23.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.23.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.23.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.23.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.23.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.24.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.24.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.24.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.24.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.24.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.25.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.25.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.25.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.25.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.25.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26', SiglipEncoderLayer(\n",
      "  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attn): SiglipAttention(\n",
      "    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): SiglipMLP(\n",
      "    (activation_fn): PytorchGELUTanh()\n",
      "    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "  )\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.26.layer_norm1', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.self_attn', SiglipAttention(\n",
      "  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.26.self_attn.k_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.self_attn.v_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.self_attn.q_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.self_attn.out_proj', Linear(in_features=1152, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.layer_norm2', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.mlp', SiglipMLP(\n",
      "  (activation_fn): PytorchGELUTanh()\n",
      "  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "))\n",
      "('vision_tower.vision_model.encoder.layers.26.mlp.activation_fn', PytorchGELUTanh())\n",
      "('vision_tower.vision_model.encoder.layers.26.mlp.fc1', Linear(in_features=1152, out_features=4304, bias=True))\n",
      "('vision_tower.vision_model.encoder.layers.26.mlp.fc2', Linear(in_features=4304, out_features=1152, bias=True))\n",
      "('vision_tower.vision_model.post_layernorm', LayerNorm((1152,), eps=1e-06, elementwise_affine=True))\n",
      "('multi_modal_projector', Gemma3MultiModalProjector(\n",
      "  (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "))\n",
      "('multi_modal_projector.mm_soft_emb_norm', Gemma3RMSNorm((1152,), eps=1e-06))\n",
      "('multi_modal_projector.avg_pool', AvgPool2d(kernel_size=4, stride=4, padding=0))\n",
      "('language_model', Gemma3TextModel(\n",
      "  (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-33): 34 x Gemma3DecoderLayer(\n",
      "      (self_attn): Gemma3Attention(\n",
      "        (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "        (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Gemma3MLP(\n",
      "        (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "        (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "        (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "        (act_fn): PytorchGELUTanh()\n",
      "      )\n",
      "      (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (rotary_emb): Gemma3RotaryEmbedding()\n",
      "  (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "))\n",
      "('language_model.embed_tokens', Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0))\n",
      "('language_model.layers', ModuleList(\n",
      "  (0-33): 34 x Gemma3DecoderLayer(\n",
      "    (self_attn): Gemma3Attention(\n",
      "      (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "      (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    )\n",
      "    (mlp): Gemma3MLP(\n",
      "      (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "      (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "      (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "      (act_fn): PytorchGELUTanh()\n",
      "    )\n",
      "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  )\n",
      "))\n",
      "('language_model.layers.0', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.0.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.0.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.0.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.0.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.0.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.0.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.0.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.0.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.0.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.0.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.0.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.0.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.0.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.0.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.0.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.0.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.1', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.1.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.1.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.1.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.1.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.1.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.1.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.1.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.1.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.1.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.1.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.1.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.1.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.1.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.1.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.1.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.1.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.2', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.2.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.2.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.2.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.2.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.2.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.2.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.2.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.2.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.2.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.2.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.2.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.2.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.2.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.2.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.2.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.2.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.3', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.3.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.3.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.3.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.3.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.3.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.3.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.3.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.3.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.3.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.3.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.3.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.3.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.3.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.3.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.3.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.3.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.4', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.4.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.4.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.4.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.4.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.4.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.4.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.4.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.4.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.4.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.4.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.4.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.4.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.4.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.4.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.4.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.4.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.5', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.5.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.5.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.5.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.5.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.5.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.5.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.5.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.5.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.5.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.5.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.5.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.5.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.5.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.5.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.5.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.5.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.6', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.6.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.6.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.6.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.6.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.6.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.6.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.6.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.6.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.6.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.6.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.6.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.6.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.6.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.6.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.6.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.6.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.7', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.7.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.7.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.7.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.7.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.7.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.7.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.7.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.7.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.7.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.7.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.7.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.7.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.7.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.7.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.7.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.7.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.8', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.8.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.8.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.8.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.8.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.8.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.8.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.8.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.8.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.8.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.8.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.8.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.8.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.8.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.8.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.8.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.8.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.9', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.9.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.9.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.9.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.9.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.9.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.9.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.9.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.9.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.9.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.9.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.9.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.9.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.9.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.9.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.9.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.9.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.10', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.10.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.10.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.10.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.10.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.10.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.10.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.10.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.10.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.10.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.10.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.10.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.10.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.10.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.10.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.10.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.10.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.11', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.11.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.11.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.11.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.11.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.11.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.11.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.11.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.11.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.11.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.11.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.11.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.11.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.11.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.11.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.11.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.11.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.12', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.12.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.12.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.12.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.12.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.12.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.12.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.12.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.12.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.12.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.12.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.12.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.12.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.12.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.12.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.12.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.12.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.13', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.13.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.13.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.13.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.13.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.13.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.13.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.13.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.13.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.13.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.13.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.13.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.13.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.13.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.13.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.13.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.13.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.14', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.14.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.14.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.14.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.14.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.14.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.14.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.14.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.14.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.14.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.14.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.14.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.14.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.14.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.14.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.14.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.14.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.15', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.15.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.15.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.15.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.15.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.15.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.15.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.15.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.15.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.15.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.15.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.15.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.15.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.15.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.15.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.15.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.15.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.16', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.16.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.16.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.16.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.16.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.16.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.16.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.16.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.16.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.16.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.16.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.16.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.16.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.16.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.16.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.16.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.16.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.17', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.17.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.17.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.17.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.17.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.17.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.17.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.17.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.17.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.17.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.17.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.17.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.17.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.17.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.17.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.17.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.17.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.18', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.18.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.18.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.18.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.18.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.18.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.18.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.18.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.18.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.18.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.18.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.18.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.18.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.18.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.18.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.18.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.18.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.19', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.19.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.19.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.19.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.19.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.19.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.19.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.19.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.19.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.19.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.19.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.19.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.19.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.19.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.19.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.19.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.19.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.20', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.20.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.20.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.20.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.20.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.20.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.20.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.20.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.20.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.20.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.20.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.20.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.20.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.20.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.20.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.20.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.20.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.21', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.21.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.21.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.21.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.21.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.21.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.21.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.21.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.21.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.21.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.21.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.21.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.21.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.21.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.21.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.21.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.21.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.22', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.22.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.22.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.22.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.22.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.22.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.22.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.22.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.22.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.22.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.22.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.22.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.22.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.22.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.22.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.22.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.22.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.23', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.23.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.23.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.23.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.23.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.23.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.23.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.23.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.23.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.23.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.23.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.23.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.23.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.23.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.23.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.23.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.23.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.24', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.24.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.24.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.24.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.24.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.24.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.24.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.24.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.24.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.24.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.24.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.24.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.24.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.24.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.24.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.24.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.24.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.25', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.25.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.25.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.25.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.25.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.25.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.25.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.25.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.25.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.25.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.25.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.25.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.25.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.25.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.25.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.25.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.25.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.26', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.26.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.26.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.26.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.26.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.26.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.26.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.26.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.26.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.26.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.26.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.26.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.26.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.26.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.26.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.26.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.26.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.27', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.27.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.27.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.27.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.27.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.27.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.27.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.27.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.27.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.27.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.27.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.27.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.27.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.27.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.27.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.27.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.27.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.28', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.28.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.28.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.28.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.28.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.28.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.28.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.28.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.28.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.28.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.28.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.28.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.28.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.28.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.28.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.28.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.28.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.29', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.29.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.29.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.29.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.29.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.29.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.29.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.29.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.29.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.29.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.29.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.29.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.29.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.29.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.29.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.29.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.29.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.30', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.30.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.30.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.30.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.30.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.30.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.30.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.30.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.30.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.30.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.30.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.30.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.30.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.30.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.30.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.30.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.30.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.31', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.31.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.31.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.31.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.31.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.31.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.31.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.31.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.31.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.31.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.31.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.31.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.31.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.31.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.31.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.31.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.31.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.32', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.32.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.32.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.32.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.32.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.32.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.32.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.32.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.32.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.32.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.32.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.32.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.32.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.32.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.32.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.32.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.32.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.33', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.33.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('language_model.layers.33.self_attn.q_proj', Linear(in_features=2560, out_features=2048, bias=False))\n",
      "('language_model.layers.33.self_attn.k_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.33.self_attn.v_proj', Linear(in_features=2560, out_features=1024, bias=False))\n",
      "('language_model.layers.33.self_attn.o_proj', Linear(in_features=2048, out_features=2560, bias=False))\n",
      "('language_model.layers.33.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.33.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('language_model.layers.33.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "  (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('language_model.layers.33.mlp.gate_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.33.mlp.up_proj', Linear(in_features=2560, out_features=10240, bias=False))\n",
      "('language_model.layers.33.mlp.down_proj', Linear(in_features=10240, out_features=2560, bias=False))\n",
      "('language_model.layers.33.mlp.act_fn', PytorchGELUTanh())\n",
      "('language_model.layers.33.input_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.33.post_attention_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.33.pre_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.layers.33.post_feedforward_layernorm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.norm', Gemma3RMSNorm((2560,), eps=1e-06))\n",
      "('language_model.rotary_emb', Gemma3RotaryEmbedding())\n",
      "('language_model.rotary_emb_local', Gemma3RotaryEmbedding())\n"
     ]
    }
   ],
   "source": [
    "# we do this to see how many attention layer there are \n",
    "for group in baseModel.named_modules():\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see it has 33 attention layer so we will freeze the first 26\n",
    "\n",
    "#this wont effect that mem taken up on the GPU but lets freze the firs 80% of layers and leave the reast to train\n",
    "for param in baseModel.language_model.embed_tokens.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "max_layer_to_freeze = 26\n",
    "for i, layer in enumerate(baseModel.language_model.layers):\n",
    "    if i <= max_layer_to_freeze:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1e3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU 0:\n",
      "  Allocated: 6.38 GB\n",
      "  Cached: 7.94 GB\n",
      "  Total: 23.67 GB\n",
      "\n",
      "GPU 1:\n",
      "  Allocated: 7.74 GB\n",
      "  Cached: 7.74 GB\n",
      "  Total: 23.67 GB\n"
     ]
    }
   ],
   "source": [
    "# We do this so that we have more room on the gpus\n",
    "baseModel.vision_tower  = baseModel.vision_tower.to(\"cpu\")\n",
    "for param in baseModel.vision_tower.parameters():\n",
    "                param.requires_grad = False\n",
    "for param in baseModel.multi_modal_projector.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "    \n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel.config.output_hidden_states = True    \n",
    "baseModel.config.use_cache = False      \n",
    "baseModel.gradient_checkpointing_enable()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma3Classifier(nn.Module):\n",
    "    def __init__(self, bmodel, hiddensize, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bmodel = bmodel\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.head = nn.Linear(hiddensize, 3).to('cuda:1')\n",
    "        self.device_placement = True\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        out = self.bmodel(input_ids)\n",
    "        hidden_state = out.hidden_states[-1]\n",
    "        embeddings = hidden_state[:, -1, :]  \n",
    "\n",
    "        embeddings = embeddings.to('cuda:1')\n",
    "\n",
    "        logits = self.head(self.dropout(embeddings))\n",
    "\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5078d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gemma3Classifier(bmodel=baseModel, dropout=0.1, hiddensize=baseModel.config.text_config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters() ,lr=0.0003)\n",
    "lossi = []\n",
    "devlossi = []\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96d08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 660,688,387\n",
      "Total parameters: 4,300,087,155\n",
      "Trainable percentage: 15.36%\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable percentage: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e96f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/kuba/.virenv/base/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accumulation_steps = 8 # 8 * 4(our small batch size due to mem constraints) = 32 new updates after\n",
    "for epoch in tqdm(range(10)):\n",
    "    model.train()\n",
    "\n",
    "    loss_total = 0\n",
    "    for i,  (X_train, y_train) in enumerate(train_loader):\n",
    "        out = model(input_ids=X_train)\n",
    "        y_train = y_train.to('cuda:1')\n",
    "        loss = criterion(out, y_train)\n",
    "        loss_total += loss.item() \n",
    "\n",
    "        loss = loss / accumulation_steps #since batch size is jsut 4\n",
    "        loss.backward()\n",
    "\n",
    "        # now we upadte every {accumulation_steps} \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # if not perfectl;y diviable the we have left over gradient we need to use\n",
    "    if (i + 1) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()        \n",
    "\n",
    "    lossi.append(loss_total / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    dev_loss_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_dev, y_dev in dev_loader:\n",
    "            out = model(input_ids=X_dev)\n",
    "            y_dev = y_dev.to('cuda:1')\n",
    "            loss = criterion(out, y_dev)\n",
    "            dev_loss_total += loss.item()\n",
    "\n",
    "    devlossi.append(dev_loss_total / len(dev_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi, label=\"lossi\")\n",
    "plt.plot(devlossi, label=\"devlossi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d518970",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = dataset['train'][4]\n",
    "ex_text = ex['text']\n",
    "ex_input = torch.tensor(ex['input_ids']).unsqueeze(dim=0)\n",
    "ex_label = ex['label']\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(ex_input)\n",
    "\n",
    "print(f'The test is: {ex_text}')\n",
    "if ex_label == 0:\n",
    "    print(f'The label is: [1, 0, 0]')\n",
    "elif ex_label == 1:\n",
    "    print(f'The label is: [0, 1, 0]')\n",
    "else:\n",
    "    print(f'The label is: [0, 0, 1]')\n",
    "\n",
    "print(f'The pred is: {torch.softmax(pred, dim=1)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
