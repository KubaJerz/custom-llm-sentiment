{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36e8274",
   "metadata": {},
   "source": [
    "Loosely following:\n",
    " \n",
    "https://www.datacamp.com/tutorial/fine-tuning-large-language-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1324b8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.virenv/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, Gemma3Model,  TrainingArguments, Trainer\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c38933",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "MODEL = \"google/gemma-3-4b-it\"\n",
    "SEED = 69\n",
    "\n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242f85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get tha dataset\n",
    "# For us the dataset will be \n",
    "raw_dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "df_train = pd.DataFrame(raw_dataset['train'])\n",
    "df_test = pd.DataFrame(raw_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158fe6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each segment of text \"tweet\" has a class 0 (negative), 1 (neutral), or 2 (positive)\n",
    "df_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edd47f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26727</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26728</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26729</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26730</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26731</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26732 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  label  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going      1   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n",
       "2      088c60f138                          my boss is bullying me...      0   \n",
       "3      9642c003ef                     what interview! leave me alone      0   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n",
       "...           ...                                                ...    ...   \n",
       "26727  4eac33d1c0   wish we could come see u on Denver  husband l...      0   \n",
       "26728  4f4c4fc327   I`ve wondered about rake to.  The client has ...      0   \n",
       "26729  f67aae2310   Yay good for both of you. Enjoy the break - y...      2   \n",
       "26730  ed167662a5                         But it was worth it  ****.      2   \n",
       "26731  6f7127d9d7     All this flirting going on - The ATG smiles...      1   \n",
       "\n",
       "      label_text  \n",
       "0        neutral  \n",
       "1       negative  \n",
       "2       negative  \n",
       "3       negative  \n",
       "4       negative  \n",
       "...          ...  \n",
       "26727   negative  \n",
       "26728   negative  \n",
       "26729   positive  \n",
       "26730   positive  \n",
       "26731    neutral  \n",
       "\n",
       "[26732 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feed4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this to format the input so model can understand\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd20ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding:  {'input_ids': [[0, 0, 0, 0, 2, 23391, 1902], [2, 236763, 13990, 1133, 531, 9039, 19406]], 'attention_mask': [[0, 0, 0, 0, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n",
      "decoding:  ['<pad><pad><pad><pad><bos>hello world', '<bos>bobby like to eat pizza']\n"
     ]
    }
   ],
   "source": [
    "# test of the tokenizer\n",
    "text = ['hello world', 'bobby like to eat pizza']\n",
    "vec = tokenizer(text, padding=True)\n",
    "print(\"encoding: \",vec)\n",
    "\n",
    "print(\"decoding: \",tokenizer.batch_decode(vec['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed4808b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we jsut define this so be used with the 'dataset' map function so apply to the data\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data['text'], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94fe379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokanizeion to the dataset\n",
    "dataset = raw_dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0052a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset and split into smaller part sow e can run on laptop\n",
    "train = dataset['train'].shuffle(SEED).select(range(4))\n",
    "test = dataset['test'].shuffle(SEED).select(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6cd2b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128]), torch.Size([4]), torch.Size([4, 128]), torch.Size([4]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make data into a tensor\n",
    "X_train = torch.tensor(train['input_ids'])\n",
    "y_train = torch.tensor(train['label'])\n",
    "X_test = torch.tensor(test['input_ids'])\n",
    "y_test = torch.tensor(test['label'])\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec73e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"\\nGPU {i}:\")\n",
    "            print(f\"  Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Total: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f31a9ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.93s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU 0:\n",
      "  Allocated: 7.93 GB\n",
      "  Cached: 7.94 GB\n",
      "  Total: 23.65 GB\n",
      "\n",
      "GPU 1:\n",
      "  Allocated: 7.74 GB\n",
      "  Cached: 7.74 GB\n",
      "  Total: 23.65 GB\n"
     ]
    }
   ],
   "source": [
    "# Since we are using gemma we need to def a model for seq classification\n",
    "# To do so we will import the base model then construct our model using output from the base model\n",
    "baseModel = Gemma3Model.from_pretrained(MODEL, device_map='auto', \n",
    "                                        output_hidden_states=True, \n",
    "                                        attn_implementation=\"eager\", \n",
    "                                            max_memory = {\n",
    "                                            0: \"20GiB\",        # CGPU 0 - more memory training\n",
    "                                            1: \"8GiB\",        # GPU 1 - less of the model since it will have outpus and y \n",
    "                                            \"cpu\": \"80Gib\"\n",
    "                                            }\n",
    "                                        )\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04ad10a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU 0:\n",
      "  Allocated: 6.38 GB\n",
      "  Cached: 7.94 GB\n",
      "  Total: 23.65 GB\n",
      "\n",
      "GPU 1:\n",
      "  Allocated: 7.74 GB\n",
      "  Cached: 7.74 GB\n",
      "  Total: 23.65 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseModel.vision_tower  = baseModel.vision_tower.to(\"cpu\")\n",
    "for param in baseModel.vision_tower.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b8b7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel.config.output_hidden_states = True          \n",
    "baseModel.gradient_checkpointing_enable()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e4264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma3Classifier(nn.Module):\n",
    "    def __init__(self, bmodel, hiddensize, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bmodel = bmodel\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.head = nn.Linear(hiddensize, 3).to('cuda:1')\n",
    "        self.device_placement = True\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        out = self.bmodel(input_ids)\n",
    "        hidden_state = out.hidden_states[-1]\n",
    "        embeddings = hidden_state[:, -1, :]  \n",
    "\n",
    "        embeddings = embeddings.to('cuda:1')\n",
    "\n",
    "        logits = self.head(self.dropout(embeddings))\n",
    "        del embeddings\n",
    "\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5078d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gemma3Classifier(bmodel=baseModel, dropout=0.1, hiddensize=baseModel.config.text_config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9882b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db67e364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4767, -1.7002, -4.2100],\n",
       "        [ 0.4149, -0.5600,  0.5343],\n",
       "        [-1.9854, -1.8955, -3.7252],\n",
       "        [ 0.2192,  0.1666, -0.7553]], device='cuda:1',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d9c1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.head.parameters() ,lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf5e96f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:55<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "y_train = y_train.to('cuda:1')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(input_ids=X_train)\n",
    "\n",
    "    loss = criterion(out, y_train.to('cuda:1'))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lossi.append(loss.item())\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf18144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.415424108505249,\n",
       " 0.6559700965881348,\n",
       " 0.3055573105812073,\n",
       " 0.10778151452541351,\n",
       " 0.07022865116596222,\n",
       " 0.0264415442943573,\n",
       " 0.010756859555840492,\n",
       " 0.007234584540128708,\n",
       " 0.005657943431288004,\n",
       " 0.0035776859149336815,\n",
       " 0.002383651677519083,\n",
       " 0.0028499371837824583,\n",
       " 0.0009930935921147466,\n",
       " 0.0007010772242210805,\n",
       " 0.0005219571758061647,\n",
       " 0.0009688568534329534,\n",
       " 0.0005515862721949816,\n",
       " 0.00031373760430142283,\n",
       " 0.00044760460150428116,\n",
       " 0.00021824135910719633,\n",
       " 0.00013935935567133129,\n",
       " 0.0001674848172115162,\n",
       " 0.00015520457236561924,\n",
       " 0.00012592134589795023,\n",
       " 0.00015762449766043574,\n",
       " 0.00014960102271288633,\n",
       " 0.00011769855336751789,\n",
       " 0.00014391285367310047,\n",
       " 0.00010238455433864146,\n",
       " 0.00010527321137487888,\n",
       " 9.487610805081204e-05,\n",
       " 0.00011474950588308275,\n",
       " 0.00016279389092233032,\n",
       " 3.6625457141781226e-05,\n",
       " 0.00011057771916966885,\n",
       " 0.00011489531607367098,\n",
       " 6.14162054262124e-05,\n",
       " 3.32279087160714e-05,\n",
       " 6.144627695903182e-05,\n",
       " 6.079075683373958e-05,\n",
       " 9.660367504693568e-05,\n",
       " 4.5475542719941586e-05,\n",
       " 3.331756670377217e-05,\n",
       " 4.4163723941892385e-05,\n",
       " 0.00010172654583584517,\n",
       " 8.581731526646763e-05,\n",
       " 6.892515375511721e-05,\n",
       " 0.00011382037337170914,\n",
       " 5.3490617574425414e-05,\n",
       " 3.0009905458427966e-05,\n",
       " 2.6373994842288084e-05,\n",
       " 4.7292640374507755e-05,\n",
       " 4.3687145080184564e-05,\n",
       " 3.2810574339237064e-05,\n",
       " 5.658930604113266e-05,\n",
       " 2.8817801648983732e-05,\n",
       " 5.557568510994315e-05,\n",
       " 3.164859663229436e-05,\n",
       " 4.833627463085577e-05,\n",
       " 8.090164192253724e-05,\n",
       " 5.158326530363411e-05,\n",
       " 5.054048233432695e-05,\n",
       " 5.962824434391223e-05,\n",
       " 7.476450264221057e-05,\n",
       " 5.0182847189716995e-05,\n",
       " 3.310866668471135e-05,\n",
       " 6.820918497396633e-05,\n",
       " 3.9635000575799495e-05,\n",
       " 3.897988790413365e-05,\n",
       " 3.984378417953849e-05,\n",
       " 4.172083208686672e-05,\n",
       " 3.7727339076809585e-05,\n",
       " 3.817443212028593e-05,\n",
       " 6.737557851010934e-05,\n",
       " 6.204152305144817e-05,\n",
       " 5.244773637969047e-05,\n",
       " 3.105245923507027e-05,\n",
       " 3.412193836993538e-05,\n",
       " 5.352041989681311e-05,\n",
       " 4.28228777309414e-05,\n",
       " 3.0575749406125396e-05,\n",
       " 4.431307388585992e-05,\n",
       " 3.0993000109447166e-05,\n",
       " 3.805525921052322e-05,\n",
       " 3.5462944651953876e-05,\n",
       " 4.833565981243737e-05,\n",
       " 2.6582567443256266e-05,\n",
       " 4.434264701558277e-05,\n",
       " 3.3406715374439955e-05,\n",
       " 4.833556158700958e-05,\n",
       " 3.1261093681678176e-05,\n",
       " 5.6202869018306956e-05,\n",
       " 2.9831131541868672e-05,\n",
       " 2.6582607461023144e-05,\n",
       " 6.111796392360702e-05,\n",
       " 2.8340795324766077e-05,\n",
       " 4.56832603958901e-05,\n",
       " 3.081406975979917e-05,\n",
       " 2.598685932753142e-05,\n",
       " 4.553455801215023e-05]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
